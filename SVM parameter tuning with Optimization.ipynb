{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/godsonajodo/cs164-svm-parameter-tuning-with-optimization?scriptVersionId=214072100\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:02:33.347226Z","iopub.execute_input":"2024-12-20T21:02:33.347524Z","iopub.status.idle":"2024-12-20T21:02:33.352712Z","shell.execute_reply.started":"2024-12-20T21:02:33.347501Z","shell.execute_reply":"2024-12-20T21:02:33.351862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --extra-index-url=https://pypi.nvidia.com cuml-cu12==24.4.* ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:02:33.353934Z","iopub.execute_input":"2024-12-20T21:02:33.354259Z","iopub.status.idle":"2024-12-20T21:04:03.972378Z","shell.execute_reply.started":"2024-12-20T21:02:33.354225Z","shell.execute_reply":"2024-12-20T21:04:03.971588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scikit-optimize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:04:03.974211Z","iopub.execute_input":"2024-12-20T21:04:03.974484Z","iopub.status.idle":"2024-12-20T21:04:07.191823Z","shell.execute_reply.started":"2024-12-20T21:04:03.974461Z","shell.execute_reply":"2024-12-20T21:04:07.19099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import Necessary Libraries\nfrom cuml import SVC\nfrom cuml.preprocessing import StandardScaler\nfrom cuml.model_selection import GridSearchCV as cumlGridSearchCV\nimport cupy as cp\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport random\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split  # Only scikit-learn's train_test_split is used\nimport time\nimport traceback\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:04:07.19344Z","iopub.execute_input":"2024-12-20T21:04:07.193703Z","iopub.status.idle":"2024-12-20T21:04:16.215389Z","shell.execute_reply.started":"2024-12-20T21:04:07.19368Z","shell.execute_reply":"2024-12-20T21:04:16.214388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef visualize_samples(X, y, predictions=None, sample_size=10, title_prefix=\"Sample\"):\n    \"\"\"\n    Visualizes a sample of images and their corresponding labels or predictions.\n    \n    Parameters:\n    - X (cupy.ndarray): Array of image data.\n    - y (cupy.ndarray or numpy.ndarray): Array of labels or predictions.\n    - predictions (cupy.ndarray or numpy.ndarray, optional): Array of predictions to compare with true labels.\n    - sample_size (int): Number of samples to visualize.\n    - title_prefix (str): Prefix for the plot titles.\n    \"\"\"\n    num_samples = X.shape[0]\n    \n    if sample_size > num_samples:\n        sample_size = num_samples\n        print(f\"Warning: sample_size was greater than the actual data size. Visualizing {sample_size} samples.\")\n    \n    rand_indices = random.sample(range(num_samples), sample_size)\n    images = X[rand_indices]\n    labels = y[rand_indices]\n    \n    plt.figure(figsize=(15, 12))\n    \n    for i in range(sample_size):\n        plt.subplot(2, 5, i + 1)\n        plt.axis('off')\n        image = cp.asnumpy(images[i]).reshape(28, 28)\n        plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n        if predictions is not None:\n            true_label = int(labels[i])\n            pred_label = int(predictions[i])\n            title = f\"True: {true_label}\\nPred: {pred_label}\"\n        else:\n            title = f\"Label: {int(labels[i])}\"\n        plt.title(title, fontsize=8)\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:04:16.216588Z","iopub.execute_input":"2024-12-20T21:04:16.217546Z","iopub.status.idle":"2024-12-20T21:04:16.224335Z","shell.execute_reply.started":"2024-12-20T21:04:16.217495Z","shell.execute_reply":"2024-12-20T21:04:16.223502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef load_and_preprocess_data():\n    \"\"\"\n    Loads the MNIST dataset, splits it into training and testing sets, and scales the features.\n    \n    Returns:\n    - X_train_scaled (cupy.ndarray): Scaled training features.\n    - X_test_scaled (cupy.ndarray): Scaled testing features.\n    - y_train (cupy.ndarray): Training labels.\n    - y_test (cupy.ndarray): Testing labels.\n    \"\"\"\n    # Load MNIST Dataset\n    print(\"Loading MNIST dataset...\")\n    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n    X, y = mnist[\"data\"], mnist[\"target\"]\n    y = y.astype(int)\n    \n    # Split into Training and Testing Sets\n    print(\"Splitting data into training and testing sets...\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.1, random_state=42, stratify=y\n    )\n    \n    # Convert to CuPy Arrays with Appropriate dtypes\n    print(\"Converting data to CuPy arrays...\")\n    X_train = cp.asarray(X_train, dtype=cp.float32)\n    X_test = cp.asarray(X_test, dtype=cp.float32)\n    y_train = cp.asarray(y_train, dtype=cp.int32)\n    y_test = cp.asarray(y_test, dtype=cp.int32)\n    \n    # Scale Features\n    print(\"Scaling features with StandardScaler...\")\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    return X_train_scaled, X_test_scaled, y_train, y_test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:04:16.225268Z","iopub.execute_input":"2024-12-20T21:04:16.225557Z","iopub.status.idle":"2024-12-20T21:04:16.474124Z","shell.execute_reply.started":"2024-12-20T21:04:16.225525Z","shell.execute_reply":"2024-12-20T21:04:16.47307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport seaborn as sns\n\ndef plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n    \"\"\"\n    Plots a confusion matrix as a heatmap.\n\n    Parameters:\n    - cm (numpy.ndarray): Confusion matrix.\n    - title (str): Title for the plot.\n\n    Returns:\n    - None\n    \"\"\"\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=range(10), yticklabels=range(10))\n    plt.title(title)\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.show()\n\ndef evaluate_model(model, X_test, y_test, title=\"Model Evaluation\"):\n    \"\"\"\n    Evaluates the trained model on the test set and prints classification metrics.\n    Additionally, displays the confusion matrix as a heatmap.\n\n    Parameters:\n    - model (cuml.svm.SVC or sklearn.svm.SVC): Trained SVM model.\n    - X_test (cupy.ndarray): Scaled testing features.\n    - y_test (cupy.ndarray): Testing labels.\n    - title (str): Title for the evaluation output.\n\n    Returns:\n    - None\n    \"\"\"\n    try:\n        print(f\"Evaluating {title}...\")\n        y_pred = model.predict(X_test)\n        \n        # Convert CuPy arrays to NumPy for evaluation and visualization\n        y_test_np = cp.asnumpy(y_test)\n        y_pred_np = cp.asnumpy(y_pred)\n        # X_test_np = cp.asnumpy(X_test)  # Not used here\n        \n        # Classification Report\n        print(f\"{title} Classification Report:\\n{metrics.classification_report(y_test_np, y_pred_np)}\")\n        \n        # Confusion Matrix\n        cm = metrics.confusion_matrix(y_test_np, y_pred_np)\n        print(f\"{title} Confusion Matrix:\\n{cm}\")\n        \n        # Plot Confusion Matrix as Heatmap\n        plot_confusion_matrix(cm, title=f\"{title} Confusion Matrix Heatmap\")\n        \n        # Visualize Some Predictions\n        visualize_samples(X_test, y_pred, sample_size=10, title_prefix=title)\n    except Exception as e:\n        print(\"Error in evaluate_model:\")\n        traceback.print_exc()\n        raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:04:16.475255Z","iopub.execute_input":"2024-12-20T21:04:16.475563Z","iopub.status.idle":"2024-12-20T21:04:16.601426Z","shell.execute_reply.started":"2024-12-20T21:04:16.475537Z","shell.execute_reply":"2024-12-20T21:04:16.600406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_baseline_model_linear(X_train, y_train):\n    \"\"\"\n    Trains a baseline SVM model with default hyperparameters using the linear kernel\n    \n    Parameters:\n    - X_train (cupy.ndarray): Scaled training features.\n    - y_train (cupy.ndarray): Training labels.\n    \n    Returns:\n    - model (cuml.svm.SVC): Trained SVM model.\n    \"\"\"\n    print(\"Training baseline SVM model with default hyperparameters using the linear kernel\")\n    model = SVC(kernel='linear', random_state=42)\n    model.fit(X_train, y_train)\n    print(\"Baseline model training complete.\\n\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:04:16.603685Z","iopub.execute_input":"2024-12-20T21:04:16.603937Z","iopub.status.idle":"2024-12-20T21:04:17.265586Z","shell.execute_reply.started":"2024-12-20T21:04:16.603916Z","shell.execute_reply":"2024-12-20T21:04:17.264442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef train_baseline_model_rbf(X_train, y_train):\n    \"\"\"\n    Trains a baseline SVM model with default hyperparameters using rbf\n    \n    Parameters:\n    - X_train (cupy.ndarray): Scaled training features.\n    - y_train (cupy.ndarray): Training labels.\n    \n    Returns:\n    - model (cuml.svm.SVC): Trained SVM model.\n    \"\"\"\n    print(\"Training baseline SVM model with default hyperparameters using rbf\")\n    model = SVC(kernel='rbf', random_state=42)\n    model.fit(X_train, y_train)\n    print(\"Baseline model training complete.\\n\")\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:04:17.267232Z","iopub.execute_input":"2024-12-20T21:04:17.267583Z","iopub.status.idle":"2024-12-20T21:04:18.32153Z","shell.execute_reply.started":"2024-12-20T21:04:17.26753Z","shell.execute_reply":"2024-12-20T21:04:18.320603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef perform_grid_search(X_train, y_train):\n    \"\"\"\n    Performs hyperparameter tuning using GridSearchCV.\n    \n    Parameters:\n    - X_train (cupy.ndarray): Scaled training features.\n    - y_train (cupy.ndarray): Training labels.\n    \n    Returns:\n    - best_model (cuml.svm.SVC): Best SVM model found by GridSearchCV.\n    - grid_search (cuml.model_selection.GridSearchCV): GridSearchCV object containing results.\n    \"\"\"\n    print(\"Starting hyperparameter tuning with GridSearchCV...\")\n    \n    # Define Parameter Grid\n    param_grid = {\n        'kernel': ['rbf'],\n        'C': [5, 10],\n        'gamma': [1e-2, 1e-3, 1e-4],\n    }\n    \n    # Initialize GridSearchCV\n    grid_search = cumlGridSearchCV(\n        estimator=SVC(random_state=42),\n        param_grid=param_grid,\n        cv=3,\n        verbose=2\n    )\n    \n    # Fit GridSearchCV\n    grid_search.fit(X_train, y_train)\n    \n    print(\"GridSearchCV completed.\")\n    print(f\"Best Parameters: {grid_search.best_params_}\")\n    print(f\"Best Score: {grid_search.best_score_}\\n\")\n    \n    return grid_search.best_estimator_, grid_search\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:04:18.322528Z","iopub.execute_input":"2024-12-20T21:04:18.322842Z","iopub.status.idle":"2024-12-20T21:04:18.795437Z","shell.execute_reply.started":"2024-12-20T21:04:18.322812Z","shell.execute_reply":"2024-12-20T21:04:18.794701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef main():\n    \"\"\"\n    Main function to orchestrate the SVM training, hyperparameter tuning, and evaluation.\n    \"\"\"\n    start_time = time.time()\n    \n    # Load and Preprocess Data\n    X_train_scaled, X_test_scaled, y_train, y_test = load_and_preprocess_data()\n    \n    # Visualize Some Training Samples\n    visualize_samples(X_train_scaled, y_train, sample_size=10, title_prefix=\"Training Sample\")\n    \n    X_train_cpu = cp.asnumpy(X_train_scaled)\n    y_train_cpu = cp.asnumpy(y_train)\n\n    \n\n    \n    # Train Baseline Model using Linear kernel\n    baseline_model_linear = train_baseline_model_linear(X_train_cpu, y_train_cpu)\n    \n    # Evaluate Baseline Model\n    evaluate_model(baseline_model_linear, X_test_scaled, y_test, title=\"Baseline Model\")\n\n    # Train Baseline Model using rbf kernel\n    baseline_model_rbf = train_baseline_model_rbf(X_train_cpu, y_train_cpu)\n    \n    # Evaluate Baseline Model\n    evaluate_model(baseline_model_rbf, X_test_scaled, y_test, title=\"Baseline Model\")\n    \n    # Perform Hyperparameter Tuning with GridSearchCV\n    best_model, grid_search = perform_grid_search(X_train_cpu, y_train_cpu)\n    \n    # Evaluate Best Model from Grid Search\n    evaluate_model(best_model, X_test_scaled, y_test, title=\"Grid Search Best Model\")\n    \n    end_time = time.time()\n    total_time = end_time - start_time\n    print(f\"Total Execution Time: {total_time:.2f} seconds\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:04:18.796232Z","iopub.execute_input":"2024-12-20T21:04:18.796522Z","iopub.status.idle":"2024-12-20T21:08:45.230646Z","shell.execute_reply.started":"2024-12-20T21:04:18.796492Z","shell.execute_reply":"2024-12-20T21:08:45.229807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def perform_bayesian_optimization(X_train, y_train):\n    \"\"\"\n    Performs hyperparameter tuning using Bayesian optimization with Optuna.\n    \n    Parameters:\n    - X_train (cupy.ndarray): Scaled training features.\n    - y_train (cupy.ndarray): Training labels.\n    \n    Returns:\n    - best_model (cuml.svm.SVC): Best SVM model found by Bayesian optimization.\n    - study (optuna.study.Study): Optuna study object containing results.\n    \"\"\"\n    print(\"Starting hyperparameter tuning with Bayesian Optimization (Optuna)...\")\n    \n    def objective(trial):\n        C = trial.suggest_float('C', 1, 20)\n        gamma = trial.suggest_float('gamma', 1e-4, 1e-2, log=True)\n        \n        model = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)\n        model.fit(X_train, y_train)\n        \n        y_pred = model.predict(X_train)\n        accuracy = metrics.accuracy_score(cp.asnumpy(y_train), cp.asnumpy(y_pred))\n        return accuracy\n    \n    study = optuna.create_study(direction='maximize')\n    study.optimize(objective, n_trials=10)\n    \n    best_params = study.best_params\n    best_model = SVC(kernel='rbf', random_state=42, **best_params)\n    best_model.fit(X_train, y_train)\n    \n    print(\"Bayesian Optimization (Optuna) completed.\")\n    print(f\"Best Parameters: {best_params}\")\n    print(f\"Best Score: {study.best_value}\\\\n\")\n    \n    return best_model, study\n\ndef perform_random_search(X_train, y_train, n_iter=10):\n    \"\"\"\n    Performs hyperparameter tuning using Random Search with skopt.\n    \n    Parameters:\n    - X_train (cupy.ndarray): Scaled training features.\n    - y_train (cupy.ndarray): Training labels.\n    - n_iter (int): Number of random combinations to try.\n    \n    Returns:\n    - best_model (cuml.svm.SVC): Best SVM model found by Random Search.\n    - random_search (skopt.BayesSearchCV): RandomSearchCV object containing results.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import pandas as pd\n    print(\"Starting hyperparameter tuning with Random Search...\")\n\n    search_spaces = {\n        'C': Real(1, 20),\n        'gamma': Real(1e-4, 1e-2, prior='log-uniform'),\n    }\n    \n    random_search = BayesSearchCV(\n        SVC(kernel='rbf', random_state=42),\n        search_spaces,\n        n_iter=n_iter,\n        cv=3,\n        n_jobs=-1\n    )\n    \n    random_search.fit(cp.asnumpy(X_train), cp.asnumpy(y_train))\n    \n    print(\"Random Search completed.\")\n    print(f\"Best Parameters: {random_search.best_params_}\")\n    print(f\"Best Score: {random_search.best_score_}\\\\n\")\n\n    best_model = SVC(kernel='rbf', random_state=42, **random_search.best_params_)\n    best_model.fit(X_train, y_train)\n\n\n\n    \n    \n    return best_model, random_search\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:08:45.231753Z","iopub.execute_input":"2024-12-20T21:08:45.232003Z","iopub.status.idle":"2024-12-20T21:08:45.240159Z","shell.execute_reply.started":"2024-12-20T21:08:45.231983Z","shell.execute_reply":"2024-12-20T21:08:45.238999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:08:45.241111Z","iopub.execute_input":"2024-12-20T21:08:45.24163Z","iopub.status.idle":"2024-12-20T21:08:48.400425Z","shell.execute_reply.started":"2024-12-20T21:08:45.241582Z","shell.execute_reply":"2024-12-20T21:08:48.399282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\n# Load and Preprocess Data\nX_train_scaled, X_test_scaled, y_train, y_test = load_and_preprocess_data()\n\n# Visualize Some Training Samples\nvisualize_samples(X_train_scaled, y_train, sample_size=10, title_prefix=\"Training Sample\")\n\nX_train_cpu = cp.asnumpy(X_train_scaled)\ny_train_cpu = cp.asnumpy(y_train)\n\n# Perform Hyperparameter Tuning with Bayesian Optimization\nbest_model_bayes, study = perform_bayesian_optimization(X_train_cpu, y_train_cpu)\n\n# Evaluate Best Model from Bayesian Optimization\nevaluate_model(best_model_bayes, X_test_scaled, y_test, title=\"Bayesian Optimization Best Model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T21:08:48.401744Z","iopub.execute_input":"2024-12-20T21:08:48.402047Z","iopub.status.idle":"2024-12-20T21:12:23.049252Z","shell.execute_reply.started":"2024-12-20T21:08:48.402023Z","shell.execute_reply":"2024-12-20T21:12:23.048414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}